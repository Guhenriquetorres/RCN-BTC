# -*- coding: utf-8 -*-
"""RCN-BTC

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13zllNepkYc3EgLDvq-wyUvJdzOCrIeuN

# **INSTALANDO DEPENDÊNCIAS**
"""

!pip install xgboost
!pip install fredapi
!pip install yfinance
!pip install tensorflow
!pip install ta
!pip install requests-cache
!pip install tensorflow
!pip uninstall -y numpy pmdarima
!pip install numpy==1.23.5
!pip install pmdarima

import yfinance as yf
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from ta.trend import EMAIndicator
from ta.momentum import RSIIndicator
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import MinMaxScaler
from fredapi import Fred
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.ensemble import RandomForestRegressor

"""# **CARREGANDO OS DADOS**

**BTC E SP500**
"""

# Fetch historical data
btc = yf.download('BTC-USD', start='2015-01-01', end='2024-01-01')
sp500 = yf.download('^GSPC', start='2015-01-01', end='2024-01-01')

# Extract closing prices and ensure indices are dates
btc_close = btc['Close']
sp500_close = sp500['Close']

btc_close.index = pd.to_datetime(btc_close.index)
sp500_close.index = pd.to_datetime(sp500_close.index)

"""**FEDERAL RESERVE**"""

# Fetch Federal Funds Rate data from FRED
fred = Fred(api_key='f2aeb0f0a807498a62d1b0d4311a431c')  # Replace with your valid FRED API key
fed_funds_rate = fred.get_series('FEDFUNDS', start='2015-01-01', end='2024-01-01')

# Ensure consistent datetime format
btc.index = btc.index.tz_localize(None)  # Remove timezone information
sp500.index = sp500.index.tz_localize(None)  # Remove timezone information
fed_funds_rate.index = fed_funds_rate.index.tz_localize(None)  # Remove timezone information

"""# **ETL DA COLUNA**"""

# Certifique-se de que cada conjunto de dados seja um DataFrame com 'Close' e 'Volume'
btc_close_volume = btc[['Close', 'Volume']].copy()  # Criar DataFrame com apenas as colunas necessárias
sp500_close_volume = sp500[['Close', 'Volume']].copy()  # Criar DataFrame com apenas as colunas necessárias
fed_funds_rate = pd.DataFrame(fed_funds_rate, columns=['FedFundsRate'])  # Garantir um DataFrame

# Renomear para evitar conflitos
btc_close_volume = btc_close_volume.rename(columns={'Close': 'BTC_Close', 'Volume': 'BTC_Volume'})
sp500_close_volume = sp500_close_volume.rename(columns={'Close': 'SP500_Close', 'Volume': 'SP500_Volume'})

# Garantir que os índices sejam datetime
btc_close_volume.index = pd.to_datetime(btc_close_volume.index).tz_localize(None)
sp500_close_volume.index = pd.to_datetime(sp500_close_volume.index).tz_localize(None)
fed_funds_rate.index = pd.to_datetime(fed_funds_rate.index)

# Remover informações de fuso horário dos índices
btc_close_volume.index = btc_close.index.tz_localize(None)
sp500_close_volume.index = sp500_close.index.tz_localize(None)
fed_funds_rate.index = fed_funds_rate.index.tz_localize(None)

# Todas no mesmo tipo
btc_close_volume.index = pd.to_datetime(btc_close.index)
sp500_close_volume.index = pd.to_datetime(sp500_close.index)
fed_funds_rate.index = pd.to_datetime(fed_funds_rate.index)

print(btc_close_volume.index.tz)        # Check timezone info for btc_close
print(sp500_close_volume.index.tz)      # Check timezone info for sp500_close
print(fed_funds_rate.index.tz)   # Check timezone info for fed_funds_rate

# Garantir datetime e remover timezone
btc_close_volume.index = pd.to_datetime(btc_close_volume.index).tz_localize(None)
sp500_close_volume.index = pd.to_datetime(sp500_close_volume.index).tz_localize(None)
fed_funds_rate.index = pd.to_datetime(fed_funds_rate.index).tz_localize(None)

# Criar intervalo de datas com base no BTC
start_date = btc_close_volume.index.min()
end_date = btc_close_volume.index.max()
all_dates = pd.date_range(start=start_date, end=end_date, freq='D')

# Reindexar todos os DataFrames na base de BTC
btc_close_volume = btc_close_volume.reindex(all_dates)
sp500_close_volume = sp500_close_volume.reindex(all_dates)
fed_funds_rate_daily = fed_funds_rate.reindex(all_dates).ffill()

# Merge BTC + SP500
merged_data = btc_close_volume.merge(sp500_close_volume, how='left', left_index=True, right_index=True)

# Interpola no tempo (preenche os buracos no meio)
cols_to_interpolate = ['SP500_Close', 'SP500_Volume']
merged_data[cols_to_interpolate] = merged_data[cols_to_interpolate].interpolate(method='time')

# Garantir que a PRIMEIRA e a ÚLTIMA linha sejam preenchidas se ficaram NaN
merged_data[cols_to_interpolate] = merged_data[cols_to_interpolate].ffill().bfill()

merged_data

# Garante datetime no índice e remove timezone
fed_funds_rate.index = pd.to_datetime(fed_funds_rate.index).tz_localize(None)
# Reindexar o FED para ter as mesmas datas do merged_data (BTC + SP500)
fed_alinhado = fed_funds_rate.reindex(merged_data.index)

# Interpolar os valores nulos do FED
fed_alinhado = fed_alinhado.interpolate(method='time')

# Preencher possíveis NaNs na primeira ou última linha
fed_alinhado = fed_alinhado.ffill().bfill()

print(merged_data.isnull().sum())
print(merged_data.head())

# 1. Achatar as colunas para formato simples
merged_data.columns = ['_'.join(col).strip() for col in merged_data.columns.values]

# 2. Agora o merge vai funcionar!
merged_data = merged_data.merge(
    fed_funds_rate_daily,
    how='left',
    left_index=True,
    right_index=True
)

merged_data.head(100)

"""# **EDA**"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
from statsmodels.tsa.seasonal import seasonal_decompose

# Desagregando multiindex para facilitar o EDA
df_flat = merged_data.copy()
df_flat.columns = ['BTC_Close', 'BTC_Volume', 'SP500_Close', 'SP500_Volume', 'FedFundsRate']

# Plotando a série temporal
plt.figure(figsize=(12, 6))
for col in df_flat.columns:
    plt.plot(df_flat.index, df_flat[col], label=col)
plt.title("Séries Temporais: Preço e Volume")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

# Verificando autocorrelação e decomposição (exemplo com BTC_Close)
plot_acf(df_flat['BTC_Close'], lags=2)
plt.title("Autocorrelação - BTC_Close")
plt.tight_layout()
plt.show()

plot_pacf(df_flat['BTC_Close'], lags=2)
plt.title("Autocorrelação Parcial - BTC_Close")
plt.tight_layout()
plt.show()


# Como temos poucos dados, a decomposição será apenas ilustrativa
result = seasonal_decompose(df_flat['BTC_Close'], model='additive', period=2)
result.plot()
plt.tight_layout()
plt.show()

from statsmodels.tsa.stattools import adfuller
result = adfuller(df_flat['BTC_Close'])
print('ADF Statistic:', result[0])
print('p-value:', result[1])
print('Critical Values:')
for key, value in result[4].items():
    print('\t{}: {}'.format(key, value))

df_diff = df_flat['BTC_Close'].diff().dropna()

from statsmodels.tsa.stattools import adfuller
result = adfuller(df_diff)

print(f'ADF Statistic: {result[0]}')
print(f'p-value: {result[1]}')

from statsmodels.tsa.statespace.sarimax import SARIMAX

# Exemplo com 1ª diferença e sem exógenas
model = SARIMAX(df_flat['BTC_Close'], order=(1, 1, 1), seasonal_order=(0, 0, 0, 0))
result = model.fit(disp=False)

forecast = result.get_forecast(steps=90)
forecast_ci = forecast.conf_int()

from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
scaled_data = scaler.fit_transform(df_flat[['BTC_Close', 'BTC_Volume', 'SP500_Close', 'SP500_Volume', 'FedFundsRate']])

from sklearn.preprocessing import StandardScaler

# Separar a variável dependente e as exógenas
y = merged_data['BTC_Close']
exog = merged_data[['SP500_Close', 'SP500_Volume', 'FedFundsRate']]

# Inicializar escaladores
scaler_y = StandardScaler()
scaler_exog = StandardScaler()

# Aplicar escalas
y_scaled = scaler_y.fit_transform(y.values.reshape(-1, 1)).flatten()
exog_scaled = scaler_exog.fit_transform(exog)

# Criar e treinar modelo SARIMAX com dados escalados
model_scaled = SARIMAX(y_scaled, exog=exog_scaled, order=(1, 1, 2))
results_scaled = model_scaled.fit(disp=False)

# Previsão de 90 dias com as últimas exógenas (repetidas)
n_forecast = 90
last_exog_scaled = exog_scaled[-1].reshape(1, -1)
exog_forecast_scaled = last_exog_scaled.repeat(n_forecast, axis=0)

forecast_scaled = results_scaled.get_forecast(steps=n_forecast, exog=exog_forecast_scaled)
forecast_mean_scaled = forecast_scaled.predicted_mean
forecast_ci_scaled = forecast_scaled.conf_int()

# Inverter a escala dos resultados
forecast_mean = scaler_y.inverse_transform(forecast_mean_scaled.reshape(-1, 1)).flatten()
forecast_ci_lower = scaler_y.inverse_transform(forecast_ci_scaled[:, 0].reshape(-1, 1)).flatten()
forecast_ci_upper = scaler_y.inverse_transform(forecast_ci_scaled[:, 1].reshape(-1, 1)).flatten()

# Preparar datas para previsão
forecast_dates = pd.date_range(start=y.index[-1] + pd.Timedelta(days=1), periods=n_forecast)

# Plotar os resultados
plt.figure(figsize=(12, 6))
plt.plot(y.index, y, label='Histórico BTC_Close')
plt.plot(forecast_dates, forecast_mean, label='Previsão Escalada SARIMAX + Exógenas', color='purple')
plt.fill_between(forecast_dates, forecast_ci_lower, forecast_ci_upper, color='purple', alpha=0.3)
plt.title('Previsão de 90 dias do BTC_Close com dados normalizados')
plt.xlabel('Data')
plt.ylabel('BTC_Close')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

# Corrigir o código para refletir corretamente os nomes das colunas

# Definir colunas
exog_vars = ['SP500_Close', 'SP500_Volume', 'FedFundsRate']
target_var = 'BTC_Close'

# Separar variáveis
y = merged_data[target_var]
X = merged_data[exog_vars]

# Escalar
scaler_X = StandardScaler()
scaler_y = StandardScaler()

X_scaled = scaler_X.fit_transform(X)
y_scaled = scaler_y.fit_transform(y.values.reshape(-1, 1)).flatten()

# Repetir últimos valores para previsão futura
X_future = X.iloc[[-1]].values.repeat(90, axis=0)
X_future_scaled = scaler_X.transform(X_future)

# Gerar datas futuras
forecast_dates = pd.date_range(start=y.index[-1] + pd.Timedelta(days=1), periods=90)

# Reajustar o modelo com sazonalidade
model_seasonal = SARIMAX(
    y_scaled,
    order=(2, 1, 2),
    seasonal_order=(1, 1, 1, 7),
    exog=X_scaled,
    enforce_stationarity=False,
    enforce_invertibility=False
)

results_seasonal = model_seasonal.fit(disp=False)

# Prever
forecast_seasonal_scaled = results_seasonal.get_forecast(steps=90, exog=X_future_scaled)
forecast_mean_scaled = forecast_seasonal_scaled.predicted_mean
forecast_ci_scaled = forecast_seasonal_scaled.conf_int()

# Inverter escala
forecast_mean = scaler_y.inverse_transform(forecast_mean_scaled.reshape(-1, 1)).flatten()
forecast_ci_lower = scaler_y.inverse_transform(forecast_ci_scaled[:, 0].reshape(-1, 1)).flatten()
forecast_ci_upper = scaler_y.inverse_transform(forecast_ci_scaled[:, 1].reshape(-1, 1)).flatten()

# Plotar resultados
plt.figure(figsize=(12, 6))
plt.plot(y.index, y, label='Histórico BTC_Close')
plt.plot(forecast_dates, forecast_mean, label='SARIMAX Sazonal + Exógenas', color='green')
plt.fill_between(forecast_dates, forecast_ci_lower, forecast_ci_upper, color='green', alpha=0.3)
plt.title('Previsão de 90 dias do BTC_Close com SARIMAX Sazonal')
plt.xlabel('Data')
plt.ylabel('BTC_Close')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

"""# **GAN**"""

!pip uninstall torch
!pip install torch==2.1.0+cpu -f https://download.pytorch.org/whl/torch_stable.html
!pip uninstall -y torch torchvision torchaudio
!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

import torch
print(torch.cuda.is_available())
print(torch.cuda.get_device_name(0) if torch.cuda.is_available() else "Sem GPU")

import torch
print(torch.cuda.is_available())  # Deve dar True
print(torch.cuda.get_device_name(0))  # Nome da GPU

"""# **DADOS ESCALADOS**





Para performar bem nos modelos

# **SEQUECIAL DENSE**
"""

from sklearn.preprocessing import StandardScaler

merged_data.dtypes

# Supondo que seu DataFrame final seja "final"
numerical_cols = merged_data.select_dtypes(include='number').columns
numerical_cols

scaler = StandardScaler()

# Salvar valores escalados num novo DataFrame
merged_data_scaler = merged_data.copy()
merged_data_scaler[numerical_cols] = scaler.fit_transform(merged_data[numerical_cols])

print(merged_data_scaler.head())

merged_data.isnull().sum()

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from keras.models import Sequential
from keras.layers import Dense

# =================== 1. BASE DE DADOS ===================
df = merged_data.copy()  # Use seu DataFrame já tratado

# ----------- Calcular Indicadores Técnicos -----------
#df['EMA_8'] = df['BTC_Close_BTC-USD'].ewm(span=8, adjust=False).mean()
#df['EMA_14'] = df['BTC_Close_BTC-USD'].ewm(span=14, adjust=False).mean()

#delta = df['BTC_Close_BTC-USD'].diff()
#gain = delta.clip(lower=0)
#loss = -delta.clip(upper=0)
#avg_gain = gain.rolling(window=14, min_periods=14).mean()
#avg_loss = loss.rolling(window=14, min_periods=14).mean()
#rs = avg_gain / avg_loss
#df['RSI'] = 100 - (100 / (1 + rs))

# =================== 2. PREPARAÇÃO DOS DADOS ===================
# Features para prever o preço do BTC no próximo dia
features = df[[
    'BTC_Close_BTC-USD',
  #  'EMA_8',
   # 'EMA_14',
    #'RSI',
    'SP500_Close_^GSPC',
    'FedFundsRate',
    'BTC_Volume_BTC-USD',
    'SP500_Volume_^GSPC'

]].shift(0)  # Shift para evitar lookahead bias

target = df['BTC_Close_BTC-USD']

# Escala os dados
scaler = MinMaxScaler()
features_scaled = scaler.fit_transform(features)

# Divide em treino e teste (80/20)
train_size = int(len(features_scaled) * 0.8)
X_train, X_test = features_scaled[:train_size], features_scaled[train_size:]
y_train, y_test = target[:train_size], target[train_size:]

# =================== 3. MODELO - REDE NEURAL ===================
model = Sequential([
    Dense(128, activation='relu', input_dim=X_train.shape[1]),
    Dense(64, activation='relu'),
    Dense(32, activation='relu'),
    Dense(1, activation='linear')
])

model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])

# Treina
history = model.fit(
    X_train, y_train,
    validation_data=(X_test, y_test),
    epochs=50,
    batch_size=32,
    verbose=1
)

# =================== 4. AVALIAÇÃO ===================
y_pred = model.predict(X_test).flatten()

mae = mean_absolute_error(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
r2 = r2_score(y_test, y_pred)

print("\n📊 Avaliação do Modelo com Rede Neural:")
print(f"🔹 MAE:   {mae:.2f}")
print(f"🔹 MSE:   {mse:.2f}")
print(f"🔹 RMSE:  {rmse:.2f}")
print(f"🔹 R²:    {r2:.4f}")

# =================== 5. VISUALIZAÇÃO ===================
plt.figure(figsize=(14, 6))
plt.plot(y_test.index, y_test, label='Preço Real')
plt.plot(y_test.index, y_pred, label='Previsto (NN)', linestyle='--')
plt.title('Previsão do Preço do Bitcoin com Rede Neural')
plt.xlabel('Data')
plt.ylabel('Preço em USD')
plt.legend()
plt.tight_layout()
plt.show()

print(f"Shape y_test: {y_test.shape}")
print(f"Shape y_pred: {y_pred.shape}")

# Última data real do seu df
ultima_data = df.index[-1]
data_alvo = pd.Timestamp("2025-06-09")

dias_faltando = (data_alvo - ultima_data).days
print(f"Dias a prever: {dias_faltando}")

# =================== 6. PREVISÕES FUTURAS ===================

n_dias = 7  # Número de dias a prever
future_predictions = []
input_features = features.iloc[-1].copy()  # Última linha conhecida

for i in range(n_dias):
    # Transforma em DataFrame para manter o formato
    input_df = pd.DataFrame([input_features])
    input_scaled = scaler.transform(input_df)

    # Faz a previsão
    next_pred = model.predict(input_scaled).flatten()[0]
    future_predictions.append(next_pred)

    # Atualiza input_features para a próxima iteração
    # A única variável que muda é 'BTC_Close_BTC-USD'
    input_features['BTC_Close_BTC-USD'] = next_pred
    # (mantemos os outros valores constantes: SP500, Fed, Volume etc.)

# Cria datas futuras a partir da última data da base
last_date = df.index[-1]
future_dates = pd.date_range(start=last_date + pd.Timedelta(days=1), periods=n_dias)

# Organiza resultados em DataFrame
df_future = pd.DataFrame({
    'Data': future_dates,
    'BTC_Previsto': future_predictions
}).set_index('Data')

# Mostra previsões
print("\n📈 Previsão de longo prazo (Rede Neural):")
print(df_future)

# =================== 7. VISUALIZAÇÃO ===================
plt.figure(figsize=(14, 6))
plt.plot(df['BTC_Close_BTC-USD'], label='Histórico')
plt.plot(df_future.index, df_future['BTC_Previsto'], label='Previsão Futura', linestyle='--')
plt.title('Previsão do Bitcoin - Curto e Longo Prazo')
plt.xlabel('Data')
plt.ylabel('Preço em USD')
plt.legend()
plt.tight_layout()
plt.show()

features = merged_data[['BTC_Close_BTC-USD', 'SP500_Close_^GSPC', 'FedFundsRate', ]].shift(1)
print(features.isnull().sum())  # Isso vai mostrar 1 NaN em cada coluna por causa do shift
print(features.head())

merged_data.isnull().sum()

from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import numpy as np
import matplotlib.pyplot as plt

# ===================== MODELO =====================
model_rf = RandomForestRegressor(n_estimators=100, random_state=42)
model_rf.fit(X_train, y_train)
y_pred_rf = model_rf.predict(X_test)

# ===================== MÉTRICAS =====================
mse_rf = mean_squared_error(y_test, y_pred_rf)
mae_rf = mean_absolute_error(y_test, y_pred_rf)
rmse_rf = np.sqrt(mse_rf)
r2_rf = r2_score(y_test, y_pred_rf)

print("📊 Avaliação do Random Forest:")
print(f"🔹 MAE:  {mae_rf:.2f}")
print(f"🔹 MSE:  {mse_rf:.2f}")
print(f"🔹 RMSE: {rmse_rf:.2f}")
print(f"🔹 R²:   {r2_rf:.4f}")

# ===================== VISUALIZAÇÃO 1: PREVISÃO =====================
plt.figure(figsize=(14, 6))
plt.plot(y_test.index, y_test, label='Preço Real', linewidth=2)
plt.plot(y_test.index, y_pred_rf, label='Preço Previsto (RF)', linestyle='--', linewidth=2)
plt.title('Previsão do Preço do Bitcoin - Random Forest')
plt.xlabel('Data')
plt.ylabel('Preço em USD')
plt.legend()
plt.tight_layout()
plt.show()

# ===================== VISUALIZAÇÃO 2: RESÍDUOS =====================
residuals = y_test - y_pred_rf
plt.figure(figsize=(12, 4))
plt.plot(y_test.index, residuals, color='gray')
plt.axhline(0, color='red', linestyle='--')
plt.title('Resíduos do Modelo Random Forest')
plt.xlabel('Data')
plt.ylabel('Erro (Real - Previsto)')
plt.tight_layout()
plt.show()

"""# **PREDIÇÕES DE HOJE**"""

print(X_train.shape)

model_rf.fit(X_train, y_train)
y_pred_rf = model_rf.predict(X_test)      # Previsões no TESTE!
print(mean_absolute_error(y_test, y_pred_rf))
print(r2_score(y_test, y_pred_rf))

"""# **LSTNM**"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from keras.models import Sequential
from keras.layers import LSTM, Dense

# ========== 1. ESCALAMENTO ==========
numerical_cols = [
    'BTC_Close_BTC-USD',
    'SP500_Close_^GSPC',
    'FedFundsRate',
    'BTC_Volume_BTC-USD',
    'SP500_Volume_^GSPC'
]

scaler_X = StandardScaler()
scaler_y = StandardScaler()

# Escalar features e target separadamente
merged_data_scaler = merged_data.copy()
merged_data_scaler[numerical_cols] = scaler_X.fit_transform(merged_data[numerical_cols])
target_scaled = scaler_y.fit_transform(merged_data[['BTC_Close_BTC-USD']])

# ================= 2. CRIAR JANELAS ==================
def create_sequences(data, target, lookback=60):
    X, y = [], []
    for i in range(lookback, len(data)):
        X.append(data[i - lookback:i])
        y.append(target[i])
    return np.array(X), np.array(y)

lookback = 120
features_scaled = merged_data_scaler[numerical_cols].values
X, y = create_sequences(features_scaled, target_scaled, lookback)

# ================ 3. DIVISÃO =================
train_size = int(len(X) * 0.8)
X_train, X_test = X[:train_size], X[train_size:]
y_train, y_test = y[:train_size], y[train_size:]

# ================ 4. MODELO LSTM ================
model = Sequential([
    LSTM(64, return_sequences=False, input_shape=(lookback, X.shape[2])),
    Dense(32, activation='relu'),
    Dense(1)
])

model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])

# ================ 5. TREINAMENTO =================
history = model.fit(
    X_train, y_train,
    validation_data=(X_test, y_test),
    epochs=50,
    batch_size=32,
    verbose=1
)

# ================ 6. AVALIAÇÃO ===================
# Previsões (em escala padronizada)
y_pred_scaled = model.predict(X_test)

# Desfaz a escala para y_pred e y_test
y_pred = scaler_y.inverse_transform(y_pred_scaled).flatten()
y_test_real = scaler_y.inverse_transform(y_test.reshape(-1, 1)).flatten()

mae = mean_absolute_error(y_test_real, y_pred)
mse = mean_squared_error(y_test_real, y_pred)
rmse = np.sqrt(mse)
r2 = r2_score(y_test_real, y_pred)

print("\n📊 Avaliação do Modelo com LSTM:")
print(f"🔹 MAE:   {mae:.2f}")
print(f"🔹 MSE:   {mse:.2f}")
print(f"🔹 RMSE:  {rmse:.2f}")
print(f"🔹 R²:    {r2:.4f}")

# ================ 7. VISUALIZAÇÃO ===================
y_test_index = merged_data.index[lookback + train_size:]

plt.figure(figsize=(14, 6))
plt.plot(y_test_index, y_test_real, label='Preço Real')
plt.plot(y_test_index, y_pred, label='Previsto (LSTM)', linestyle='--')
plt.title('Previsão do Preço do Bitcoin com LSTM')
plt.xlabel('Data')
plt.ylabel('Preço em USD')
plt.legend()
plt.tight_layout()
plt.show()

"""# **LSTM2**"""

# LSTM otimizado para previsão de retorno percentual do Bitcoin
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from keras.models import Sequential
from keras.layers import LSTM, Dense, Dropout
from keras.callbacks import EarlyStopping

# ========== 1. ESCALAMENTO E FEATURE SELECTION ==========
numerical_cols = ['BTC_Close_BTC-USD', 'SP500_Close_^GSPC']

scaler_X = StandardScaler()
scaler_y = StandardScaler()

merged_data['BTC_Return'] = merged_data['BTC_Close_BTC-USD'].pct_change().shift(-1)
merged_data = merged_data.dropna()

# Escalar features e target separadamente
merged_data_scaled = merged_data.copy()
merged_data_scaled[numerical_cols] = scaler_X.fit_transform(merged_data[numerical_cols])
target_scaled = scaler_y.fit_transform(merged_data[['BTC_Return']])

# ========== 2. CRIAR JANELAS ==========
def create_sequences(data, target, lookback=60):
    X, y = [], []
    for i in range(lookback, len(data)):
        X.append(data[i - lookback:i])
        y.append(target[i])
    return np.array(X), np.array(y)

lookback = 60
features_scaled = merged_data_scaled[numerical_cols].values
X, y = create_sequences(features_scaled, target_scaled, lookback)

# ========== 3. DIVISÃO ==========
train_size = int(len(X) * 0.8)
X_train, X_test = X[:train_size], X[train_size:]
y_train, y_test = y[:train_size], y[train_size:]

# ========== 4. MODELO LSTM ==========
model = Sequential([
    LSTM(32, return_sequences=True, input_shape=(lookback, X.shape[2])),
    Dropout(0.3),
    LSTM(32),
    Dropout(0.3),
    Dense(1)
])

model.compile(optimizer='adam', loss='mae', metrics=['mse'])

# ========== 5. EARLY STOPPING ==========
early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1)

# ========== 6. TREINAMENTO ==========
history = model.fit(
    X_train, y_train,
    validation_data=(X_test, y_test),
    epochs=100,
    batch_size=32,
    callbacks=[early_stop],
    verbose=1
)

# ========== 7. AVALIAÇÃO ==========
y_pred_scaled = model.predict(X_test)
y_pred = scaler_y.inverse_transform(y_pred_scaled).flatten()
y_test_real = scaler_y.inverse_transform(y_test.reshape(-1, 1)).flatten()

mae = mean_absolute_error(y_test_real, y_pred)
mse = mean_squared_error(y_test_real, y_pred)
rmse = np.sqrt(mse)
r2 = r2_score(y_test_real, y_pred)

acertos = np.sum((y_pred > 0) == (y_test_real > 0))
acuracia_direcional = acertos / len(y_pred)

print("\n\U0001F4CA Avaliação do Modelo com LSTM:")
print(f"\U0001F539 MAE:   {mae:.6f}")
print(f"\U0001F539 MSE:   {mse:.6f}")
print(f"\U0001F539 RMSE:  {rmse:.6f}")
print(f"\U0001F539 R²:    {r2:.4f}")
print(f"\U0001F539 Acurácia Direcional: {acuracia_direcional:.2%}")

# ========== 8. VISUALIZAÇÃO DA PREVISÃO ==========
y_test_index = merged_data.index[lookback + train_size:]

plt.figure(figsize=(14, 6))
plt.plot(y_test_index, y_test_real, label='Retorno Real')
plt.plot(y_test_index, y_pred, label='Previsto (LSTM)', linestyle='--')
plt.title('Previsão de Retorno Percentual do Bitcoin com LSTM')
plt.xlabel('Data')
plt.ylabel('Retorno (%)')
plt.legend()
plt.tight_layout()
plt.show()

# ========== 9. CURVA DE PERDA ==========
plt.figure(figsize=(10, 4))
plt.plot(history.history['loss'], label='Loss - Treino')
plt.plot(history.history['val_loss'], label='Loss - Validação')
plt.title("Curva de Perda durante o Treinamento")
plt.xlabel("Épocas")
plt.ylabel("Loss")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

